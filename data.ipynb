{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r'data\\train\\masks'\n",
    "output_dir = r'data\\train\\labels'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15008_2_mask.png'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(input_dir)[:1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(input_dir, os.listdir(input_dir)[:1][0])\n",
    "\n",
    "mask = cv2.imread(image_path)\n",
    "\n",
    "cv2.imshow('mask', mask)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'15008_2_mask.png'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(input_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15008_2_mask.png']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(input_dir)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\train\\masks 15008_2_mask.png\n",
      "512 512\n",
      "data\\train\\masks 15020_0_mask.png\n",
      "512 512\n",
      "data\\train\\masks 15039_0_mask.png\n",
      "512 512\n",
      "data\\train\\masks 15069_1_mask.png\n",
      "512 512\n",
      "data\\train\\masks 15085_1_mask.png\n",
      "512 512\n"
     ]
    }
   ],
   "source": [
    "for j in os.listdir(input_dir)[:5]:\n",
    "    image_path = os.path.join(input_dir, j)\n",
    "    print(input_dir, j)\n",
    "    # load the binary mask and get its contours\n",
    "    mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    H, W = mask.shape\n",
    "    print(H, W)\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # convert the contours to polygons\n",
    "    polygons = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 200:\n",
    "            polygon = []\n",
    "            for point in cnt:\n",
    "                x, y = point[0]\n",
    "                polygon.append(x / W)\n",
    "                polygon.append(y / H)\n",
    "            polygons.append(polygon)\n",
    "\n",
    "    # print the polygons\n",
    "    with open('{}.txt'.format(os.path.join(output_dir, j)[:-4]), 'w') as f:\n",
    "        for polygon in polygons:\n",
    "            for p_, p in enumerate(polygon):\n",
    "                if p_ == len(polygon) - 1:\n",
    "                    f.write('{}\\n'.format(p))\n",
    "                elif p_ == 0:\n",
    "                    f.write('0 {} '.format(p))\n",
    "                else:\n",
    "                    f.write('{} '.format(p))\n",
    "\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data\\train\\masks\\15008_2_mask.png\n",
      "Image dimensions: 512x512\n",
      "Processing data\\train\\masks\\15020_0_mask.png\n",
      "Image dimensions: 512x512\n",
      "Processing data\\train\\masks\\15039_0_mask.png\n",
      "Image dimensions: 512x512\n",
      "Processing data\\train\\masks\\15069_1_mask.png\n",
      "Image dimensions: 512x512\n",
      "Processing data\\train\\masks\\15085_1_mask.png\n",
      "Image dimensions: 512x512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "input_dir = r'data\\train\\masks'\n",
    "output_dir = r'data\\train\\labels'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for j in os.listdir(input_dir)[:5]:\n",
    "    image_path = os.path.join(input_dir, j)\n",
    "\n",
    "    mask = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    H, W = mask.shape\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Initialize list for polygons\n",
    "    polygons = []\n",
    "\n",
    "    # Iterate over the contours\n",
    "    for idx, cnt in enumerate(contours):\n",
    "        # Process only significant contours\n",
    "        if cv2.contourArea(cnt) > 200:\n",
    "            polygon = []\n",
    "            # Normalize coordinates and add to polygon list\n",
    "            for point in cnt:\n",
    "                x, y = point[0]\n",
    "                polygon.append(x / W)\n",
    "                polygon.append(y / H)\n",
    "            # Append polygon based on its hierarchy (external or internal)\n",
    "            if hierarchy[0][idx][3] == -1:  # External contour (glasses frame)\n",
    "                polygons.append(polygon)\n",
    "            else:  # Internal contour (lenses)\n",
    "                polygons.append(polygon)\n",
    "\n",
    "    # Write the polygons to a text file\n",
    "    output_path = os.path.join(output_dir, f\"{os.path.splitext(j)[0]}.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for polygon in polygons:\n",
    "            for p_index, p in enumerate(polygon):\n",
    "                if p_index == 0:\n",
    "                    f.write(f'0 {p} ')\n",
    "                elif p_index == len(polygon) - 1:\n",
    "                    f.write(f'{p}\\n')\n",
    "                else:\n",
    "                    f.write(f'{p} ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_dir = r'data2\\val\\masks'\n",
    "labels_dir = r'data2\\val\\labels'\n",
    "imgs_dir = r'data2\\val\\images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(labels_dir, exist_ok=True)\n",
    "\n",
    "for mask_name, img_name  in tqdm(zip(os.listdir(mask_dir), os.listdir(imgs_dir))):\n",
    "    mask_path = os.path.join(mask_dir, mask_name)\n",
    "\n",
    "    mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "    _, mask = cv2.threshold(mask, 1, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    H, W = mask.shape\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(mask, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    external_polygons = []\n",
    "    internal_polygons = []\n",
    "\n",
    "    for idx, cnt in enumerate(contours):\n",
    "        if cv2.contourArea(cnt) > 200:\n",
    "            polygon = []\n",
    "            for point in cnt:\n",
    "                x, y = point[0]\n",
    "                polygon.append(x / W)\n",
    "                polygon.append(y / H)\n",
    "            if hierarchy[0][idx][3] == -1: \n",
    "                external_polygons.append(polygon)\n",
    "            else:  \n",
    "                internal_polygons.append(polygon)\n",
    "\n",
    "    output_path = os.path.join(labels_dir, f\"{img_name[:-4]}.txt\")\n",
    "    with open(output_path, 'w') as f:\n",
    "        for polygon in external_polygons:\n",
    "            f.write('0 ')\n",
    "            f.write(' '.join(map(str, polygon)))\n",
    "            f.write('\\n')\n",
    "        for polygon in internal_polygons:\n",
    "            f.write('1 ')\n",
    "            f.write(' '.join(map(str, polygon)))\n",
    "            f.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_polygons(image_shape, txt_file):\n",
    "    \"\"\"\n",
    "    Visualize polygons on a blank image.\n",
    "\n",
    "    Parameters:\n",
    "    - image_shape: tuple, the shape of the output image (height, width)\n",
    "    - txt_file: str, path to the text file containing polygon data\n",
    "    - output_image_path: str, path to save the output image with visualized polygons\n",
    "    \"\"\"\n",
    "    H, W = image_shape\n",
    "    image = np.zeros((H, W, 3), dtype=np.uint8)\n",
    "    \n",
    "    with open(txt_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip():\n",
    "            points = list(map(float, line.strip().split()[1:]))\n",
    "            polygon = [(int(points[i] * W), int(points[i+1] * H)) for i in range(0, len(points), 2)]\n",
    "            \n",
    "            print(line.strip().split()[0])\n",
    "            if line.strip().split()[0] == '0':\n",
    "                color = (0, 255, 0)\n",
    "            else:\n",
    "                color = (255, 0, 0)\n",
    "            cv2.polylines(image, [np.array(polygon, dtype=np.int32)], isClosed=True, color=color, thickness=2)\n",
    "    \n",
    "    cv2.imshow('polygon', image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_polygons((512, 512), r'data\\train\\labels\\15085_1_mask.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_yaml(dataset_path, train_dir, val_dir, class_names, output_file):\n",
    "    \"\"\"\n",
    "    Create a dataset.yaml file for YOLO training.\n",
    "    \n",
    "    :param dataset_path: Path to the dataset directory.\n",
    "    :param train_dir: Relative path to the training images directory.\n",
    "    :param val_dir: Relative path to the validation images directory.\n",
    "    :param class_names: List of class names.\n",
    "    :param output_file: Path to the output yaml file.\n",
    "    \"\"\"\n",
    "    data = {\n",
    "        'path': dataset_path,\n",
    "        'train': train_dir,\n",
    "        'val': val_dir,\n",
    "        'nc': len(class_names),\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w') as file:\n",
    "        yaml.dump(data, file, default_flow_style=False)\n",
    "    print(f\"Created {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset.yaml\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r'/kaggle/input/glasses-segm/data'\n",
    "train_dir = r'images/train'\n",
    "val_dir = r'images/val'\n",
    "class_names = ['glasses', 'lenses'] \n",
    "output_file = os.path.join('dataset.yaml')\n",
    "\n",
    "create_dataset_yaml(dataset_path, train_dir, val_dir, class_names, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
